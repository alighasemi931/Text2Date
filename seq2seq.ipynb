{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"date.csv\").drop_duplicates(subset = \"informal_date\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard\n",
    "!pip install comet-ml\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normalize and split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/Text2Date/date.csv\").drop_duplicates(subset = \"informal_date\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert Persian numbers to Latin\n",
    "    persian_to_latin = {\n",
    "        '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',\n",
    "        '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9'\n",
    "    }\n",
    "    for persian, latin in persian_to_latin.items():\n",
    "        text = text.replace(persian, latin)\n",
    "\n",
    "    # Normalize Persian characters\n",
    "    text = text.replace('ي', 'ی').replace('ك', 'ک')\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "df['formal_date'] = df['formal_date'].apply(normalize_text)\n",
    "df['informal_date'] = df['informal_date'].apply(normalize_text)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(df)\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# 2. Tokenization\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"convert date: \" + text for text in examples['informal_date']]\n",
    "    targets = examples['formal_date']\n",
    "    model_inputs = tokenizer(inputs, max_length=32, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=32, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 3. Split the Dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Initialization\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# 5. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps =5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    torch_empty_cache_steps =5,\n",
    "    warmup_steps=500\n",
    ")\n",
    "# 6. Trainer Initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 7. Model Training\n",
    "trainer.train()\n",
    "\n",
    "# 8. Evaluation\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_date(informal_date):\n",
    "    # preparing input\n",
    "    input_text = \"convert date: \" + informal_date\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # generate output\n",
    "    output = model.generate(input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "# examples\n",
    "test_dates = [\"12 اردیبهشت 1356\", \"سال چهارم شهریور 1325\", \"1392-05-03\"]\n",
    "for date in test_dates:\n",
    "    print(f\"تاریخ غیررسمی: {date} -> تاریخ رسمی: {predict_date(date)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "half_test_size = len(test_dataset) // 4\n",
    "small_test_dataset = test_dataset.select(range(half_test_size))\n",
    "predictions = trainer.predict(small_test_dataset)\n",
    "pred_labels = predictions.predictions[0].argmax(-1) \n",
    "true_labels = predictions.label_ids  \n",
    "\n",
    "pred_labels_flat = pred_labels.flatten()\n",
    "true_labels_flat = true_labels.flatten()\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "f1 = f1_score(true_labels_flat, pred_labels_flat, average='weighted')\n",
    "\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
