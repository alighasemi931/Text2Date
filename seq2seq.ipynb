{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9251, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"date.csv\").drop_duplicates(subset = \"informal_date\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normalize and split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal_date</th>\n",
       "      <th>formal_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>پنجم اردیبهشت 1402</td>\n",
       "      <td>1402/02/05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17 آبان 1395</td>\n",
       "      <td>1395/08/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 1403 مهر</td>\n",
       "      <td>1403/07/01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سال مهر اول 1403</td>\n",
       "      <td>1403/07/01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1403-07-01</td>\n",
       "      <td>1403/07/01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29435</th>\n",
       "      <td>پانزدهم آبان 1307</td>\n",
       "      <td>1307/08/15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29436</th>\n",
       "      <td>هجدهم آذر 1306</td>\n",
       "      <td>1306/09/18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29437</th>\n",
       "      <td>بیست و دوم دی 1305</td>\n",
       "      <td>1305/10/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29439</th>\n",
       "      <td>پنجم اسفند 1303</td>\n",
       "      <td>1303/12/05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29441</th>\n",
       "      <td>سی و یکم اردیبهشت 1301</td>\n",
       "      <td>1301/02/31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9251 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                informal_date formal_date\n",
       "0          پنجم اردیبهشت 1402  1402/02/05\n",
       "1                17 آبان 1395  1395/08/17\n",
       "2                  1 1403 مهر  1403/07/01\n",
       "3            سال مهر اول 1403  1403/07/01\n",
       "4                  1403-07-01  1403/07/01\n",
       "...                       ...         ...\n",
       "29435       پانزدهم آبان 1307  1307/08/15\n",
       "29436          هجدهم آذر 1306  1306/09/18\n",
       "29437      بیست و دوم دی 1305  1305/10/22\n",
       "29439         پنجم اسفند 1303  1303/12/05\n",
       "29441  سی و یکم اردیبهشت 1301  1301/02/31\n",
       "\n",
       "[9251 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert Persian numbers to Latin\n",
    "    persian_to_latin = {\n",
    "        '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',\n",
    "        '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9'\n",
    "    }\n",
    "    for persian, latin in persian_to_latin.items():\n",
    "        text = text.replace(persian, latin)\n",
    "    \n",
    "    # Normalize Persian characters\n",
    "    text = text.replace('ي', 'ی').replace('ك', 'ک')\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    # text = f\"<start> {text} <ends>\"\n",
    "    return text\n",
    "\n",
    "df['formal_date'] = df['formal_date'].apply(normalize_text)\n",
    "df['informal_date'] = df['informal_date'].apply(normalize_text)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val , test = train_test_split(df , test_size=0.1 , random_state= 42)\n",
    "train ,val = train_test_split(train_val,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7492\n",
      "Validation samples: 833\n",
      "Test samples: 926\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 8.6027\n",
      "Epoch 2/5, Loss: 8.0952\n",
      "Epoch 3/5, Loss: 7.4294\n",
      "Epoch 4/5, Loss: 6.9368\n",
      "Epoch 5/5, Loss: 6.5909\n",
      "Training complete!\n",
      "Test Accuracy: 0.0000\n",
      "Input: <start> 1 1403 مهر <ends>\n",
      "Converted: 1403/01/01 1396/06/01 1400/03/01 1400/10/01 1400/03/01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self, informal_dates, formal_dates, informal_vocab, formal_vocab):\n",
    "        self.informal_dates = informal_dates\n",
    "        self.formal_dates = formal_dates\n",
    "        self.informal_vocab = informal_vocab\n",
    "        self.formal_vocab = formal_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.informal_dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        informal = [self.informal_vocab.get(token, self.informal_vocab['<UNK>']) for token in self.informal_dates[idx].split()]\n",
    "        formal = [self.formal_vocab.get(token, self.formal_vocab['<UNK>']) for token in self.formal_dates[idx].split()]\n",
    "        return torch.tensor(informal), torch.tensor(formal)\n",
    "\n",
    "def create_vocab(texts):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1, '<start>': 2, '<ends>': 3}\n",
    "    for text in texts:\n",
    "        for token in text.split():\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    informal, formal = zip(*batch)\n",
    "    \n",
    "    # Find max length in this batch\n",
    "    max_len = max(max(len(seq) for seq in informal), max(len(seq) for seq in formal))\n",
    "    \n",
    "    # Pad sequences to max_len\n",
    "    informal_padded = [torch.cat([seq, torch.zeros(max_len - len(seq), dtype=torch.long)]) for seq in informal]\n",
    "    formal_padded = [torch.cat([seq, torch.zeros(max_len - len(seq), dtype=torch.long)]) for seq in formal]\n",
    "    \n",
    "    return torch.stack(informal_padded), torch.stack(formal_padded)\n",
    "\n",
    "class DateConversionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.1):\n",
    "        super(DateConversionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "# Prepare data\n",
    "informal_vocab = create_vocab(df['informal_date'])\n",
    "formal_vocab = create_vocab(df['formal_date'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['informal_date'], df['formal_date'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = DateDataset(X_train.tolist(), y_train.tolist(), informal_vocab, formal_vocab)\n",
    "test_dataset = DateDataset(X_test.tolist(), y_test.tolist(), informal_vocab, formal_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(informal_vocab)\n",
    "output_size = len(formal_vocab)\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize model\n",
    "model = DateConversionLSTM(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is the pad_token_id\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for informal, formal in train_loader:\n",
    "        informal, formal = informal.to(device), formal.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(informal)\n",
    "        loss = criterion(output.contiguous().view(-1, output.size(-1)), formal.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for informal, formal in test_loader:\n",
    "        informal, formal = informal.to(device), formal.to(device)\n",
    "        output = model(informal)\n",
    "        _, predicted = torch.max(output, dim=2)\n",
    "        correct += (predicted == formal).sum().item()\n",
    "        total += formal.numel()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Function to convert a single date\n",
    "def convert_date(model, input_text, informal_vocab, formal_vocab):\n",
    "    model.eval()\n",
    "    input_tokens = [informal_vocab.get(token, informal_vocab['<UNK>']) for token in input_text.split()]\n",
    "    input_tensor = torch.tensor([input_tokens]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    _, predicted = torch.max(output, dim=2)\n",
    "    predicted_tokens = predicted[0].tolist()\n",
    "    \n",
    "    rev_formal_vocab = {v: k for k, v in formal_vocab.items()}\n",
    "    converted_date = ' '.join([rev_formal_vocab[token] for token in predicted_tokens if token != 0])\n",
    "    return converted_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start> 23 1385 شهریور <ends>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m converted_date \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_date\u001b[49m(model, input_date, informal_vocab, formal_vocab)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverted_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_date' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_date = \"<start> 23 1385 شهریور <ends>\"\n",
    "converted_date = convert_date(model, input_date, informal_vocab, formal_vocab)\n",
    "print(f\"Input: {input_date}\")\n",
    "print(f\"Converted: {converted_date}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
